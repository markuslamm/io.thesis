\chapter{Implementation}
%Eigenschaften des Algorithmus, Komplexität, Wie gehen Sie vor?, Beschreiben Sie was wann passiert

After Chapter 4 "System Architecture" has shown an overview of the main components of
the software solution and explained the interaction between them, this chapter explains implementation
details for all major components of the system architecture, including several
collector implementations, as well as the "CollectorClient" and "CollectorManager" component.

The system architecture of the collector platform consists two software components to
be implemented, the Elasticsearch database, Apache Kafka message broker and the Logstash
indexer to be configured to fulfil the requirements discussed in Chapter 3 and to realize the
proposed system architecture in Chapter 4.

The main component, the CollectorClient must provide a REST interface for starting/stopping the collection process
on source systems. Furthermore, it must be possible to fetch metadata about each client. The CollectorManager,
the second software component uses this interface for providing a basic web based UI that lists registered
CollectorClients, shows detailed client information and allows the scheduling the collection process separately
for each client. It follows that two web applications are required, the client application must also be able to
send data to a Apache Kafka, hence the usage of the Procucer API of Kafka must be supported.

The choice for implementing the web applications fell on Spring Boot in the current version 1.4.0. Taken from
the reference documentation \cite{SpringB16}, "Spring Boot makes it easy to create stand-alone, production-grade
Spring based Applications that you can "just run". We take an opinionated view of the Spring platform and
third-party libraries so you can get started with minimum fuss". Features of Spring Boot include the ability to create
standalone web applications conaining an embedded servlet container, what makes the deployment of war files obsolete
and multiple integrations for different applications platforms including Apache Kafka in the subproject spring-kafka.

The following code shows a full example of an web application that provides a simple
HTTP endpoint returning "Hello World". It creates an executable jar file containing
an embedded Apache Tomcat servlet container and can be started from the command line, what means that there is no
dedicated Tomcat instance required to deploy a war file to:

\begin{lstlisting}[caption={Spring Boot "Hello World"}, captionpos=b, label={lst:spring-boot-hello-world}]
@Controller
@EnableAutoConfiguration
public class SampleController {

    @RequestMapping("/")
    @ResponseBody
    String home() {
        return "Hello World!";
    }

    public static void main(String[] args) throws Exception {
        SpringApplication.run(SampleController.class, args);
    }
}
\end{lstlisting}

The Spring framework provides usefull default configurations, thereby making it possible to create a simple Rest service
with just a few annotations. The result is a completely self-contained executable jar, created with the Maven
Buildmanagement tool. Executable jars (sometimes called “fat jars”) are archives containing the compiled classes along with
all of the jar dependencies that the code needs to run. This produces the disadvantage that the memory requirement of the
resulting executable increases. But this was ignored while making the decision for the framework because the presence of
sufficient memory space on Apache Flink and Apache Kafka sources systems had been assumed.

For the implementation of the software components, Java in its version 8 had been chosen. In this current version,
it supports more functional elements in form of lambda expressions, the processing of collections as Streams as well
as an Optional type for handling optional values respectively null values, all features which were used often in the
implementation of the CollectorClient and CollectorManager. Java is the main programming language of Spring Boot, but also
supports Groovy and Scala which did not come into consideration due to the authors lack of experience in these programming
languages.

The software-solution uses Maven as Build- and Dependencymanagement tool, and is divided into the main modules:

\begin{itemize}
	\item collectors
	\item collector-client
	\item collector-manager
	\item collector-data-processor
\end{itemize}

The following sections explains the content and and discusses implementation details for each of these modules separately.

\section{Collectors}

The collector module contains the implementations for the required collectors based on Table 3.4 in
Section 3.1 "Data Quality". It also defines a "collector-commons" module containing basic artifarcts
of the collector domain, common classes and interfaces, that are used and required by the individual
collector implementations

\subsection{Base Domain}

The next class diagramm shows the basic structure, that is equal to all collector implementations on the example
of "JvmCollector":
\begin{figure}[H]
	\centering
	\includegraphics[width=1.0\textwidth]{../uml/class-jvm-collector.jpg}
	\caption{Class diagram 'JvmCollector'}
	\label{class-diagram-jvm-collector}
\end{figure}

This overview describes the collector domain, that is equal to all implementations and consists of following parts:
\begin{enumerate}
    \item \textbf{CollectorType:}
    Enumeration, meta information, that distinguishes collectors implementations
    \item \textbf{CollectorResult:} Data container, the result of a single invocation of the collect() method from
    the Collector interface. Describes a data event, an immutable fact" at a given time, containing the host and port
    the client is running on, the type of collector and the requested data.
    \item \textbf{Collector:} The main interface for implementations, defines the protocol for data collection.
    \item \textbf{AbstractCollector:} Abstract class, provides the implementation of the main collect() method.
    \item \textbf{SampleCollector:} An interface for all classes that collect samples defined by the group the data
    belongs to.
\end{enumerate}

On the example of "JvmCollector", the following section explains the basic design principles that is common
to the main collector domain more in detail wheras only relevant implementation details will be discussed in
subsequent implementations, because the process of collecting data is similar in all.

\subsection{JvmCollector}

The implementation of the Collector interface for fetching "default" JVM data according to Table 3.2 in
Section 3.1.2 "Application Data". This basic set of different management interfaces build the foundation
for the implementation for collecting JVM related data like memory, garbage collector or thread information.

To keep the main implementation as small as possible and following the "Separation Of Concerns" pattern,
the data was divided into different "sample groups", which resulted in one implementation each for collecting
memory, garbage collector, thread data, et al.

A these SampleCollector implementations provide a collectSample() method, that fetches data from a different data group, here
on the example of the class "MemorySampleCollector".

\begin{lstlisting}[caption={MemorySampleCollector collectSample()}, captionpos=b, label={lst:memory-sample-collect}]
private static final String OBJECT_NAME = "java.lang:type=Memory";

@Override
public CompletableFuture<Map<String, Object>> collectSample() {
    return CompletableFuture.supplyAsync(() -> {
        final Map<String, Object> memoryResultMap = Maps.newLinkedHashMap();
        memoryResultMap.put(SAMPLE_KEY, parseMemory(getMemoryMXBean(OBJECT_NAME)));
        return memoryResultMap;
    });
}

private Map<String, Object> parseMemory(final MemoryMXBean proxy) {
    final Map<String, Object> memoryDataMap = Maps.newLinkedHashMap();
    memoryDataMap.put(MEMORY_OPFC_KEY, proxy.getObjectPendingFinalizationCount());
    return memoryDataMap;
}

private MemoryMXBean getMemoryMXBean(final String objectName) {
    try {
        return ManagementFactory.newPlatformMXBeanProxy(mBeanServerConnection(), objectName, MemoryMXBean.class);
    } catch (IOException ex) {
        throw ex;
    }
}
\end{lstlisting}

As one of the non functional requirements in Chapter 3, the client implementation may not cause a negative impact
on source systems regarding system resources like cpu or disk usage. To make the code as "non-blocking" as possible,
the implementation is based on the usage of CompletableFutures. It models an asynchronous computation and provides
a reference to its results that will be available when the computation itself is completed. These computations like the JMX
access in the example above, are potentially time-consuming \cite{TODO}. The CompletableFuture allows the caller thread to return immediately
to perform other operations instead of waiting for the result of the computation of JMX memory data, by delegating to a
separate thread performing the operations defined in the CompletableFuture.

The code in line 1-8 wraps the computation of JMX memory data, which contains the data access via the MBeanServerConnection,
introduced in Chapter 2 "Basic Concepts", and the preparation of result data in a separate thread performing the operations defined
in the futures method body.

The implementations for the other SampleCollectors are almost the same, it only differs in the JMX management bean the data will
be queried from.

Every collector implementation is based on an internal registry of SampleCollectors, every implementation must provide its
own registry required for the main collector process, that delegates to its registered SampleCollectors,
aggregates their results and creates the overall result of the collector. To split the collection into individual sample groups has
the advantage, that it makes the implementation very flexible regarding the data to collect. To add further data sources, it
will be enough to provide an implementation of the SampleCollector interface and to add an entry to the registry.

\begin{lstlisting}[caption={Sample registry for "JvmCollector"}, captionpos=b, label={lst:jvmsampleregistry}]
private static Map<String, SampleCollector> jvmSampleRegistry(final MBeanServerConnection mBeanServerConnection) {
    final Map<String, SampleCollector> registry = Maps.newHashMap();
    registry.put(MemorySampleCollector.SAMPLE_KEY, new MemorySampleCollector(mBeanServerConnection));
    registry.put(ThreadSampleCollector.SAMPLE_KEY, new ThreadSampleCollector(mBeanServerConnection));
    ...
    return registry;
}

@Override
public CollectorType getCollectorType() {
    return CollectorType.JVM_JMX;
}

@Override
protected void checkRegistry() {
    if (getSampleRegistry().isEmpty()) {
        throw new JmxCollectorException();
    }
}
\end{lstlisting}

The JvmCollector implementation only provides the sample registry, the type of collector as well as a simple check if sample collectors
are registered at all. These methods are defined in AbstractCollector, discussed in the next section.

\subsection{AbstractCollector}

The abstract base class for all collector implementations that contains the registry of SampleCollectors to be used in the main
collection process shown below.

\begin{lstlisting}[caption={"AbstractCollector" sample registry}, captionpos=b, label={lst:abstract-collectorsample-registry}]
private final Map<String, SampleCollector> sampleRegistry;
\end{lstlisting}

The collection process, defined by the collect() method in the Collector interface is implemented in this abstract class and
therefore the same for all implementations. The only requirement is to provide implementations for the abstract methods in this class.
The following steps are performed:
\begin{lstlisting}[caption={"AbstractCollector" Fetch sample futures}, captionpos=b, label={lst:abstract-collector-step-one}]
final List<CompletableFuture<Map<String, Object>>> sampleResultCPList = getSampleRegistry().values()
    .stream()
    .map(SampleCollector::collectSample)
    .collect(Collectors.toList());
\end{lstlisting}

This demonstrates the Stream features available since Java 8, that enable the processing of collections in a
more functional manner more focussed on the data transformations rather than the data iteself. The
implementation creates a Stream of registered SampleCollectors, collect the data for each sample and create a list of containing
futures, regarding to to SampleCollector interface.

To merge the computations from multiple SampleCollectors and extract the the data from CompletableFutures:

\begin{lstlisting}[caption={"AbstractCollector" Merge and extract data }, captionpos=b, label={lst:abstract-collector-step-two}]
final List<Map<String,Object>> sampleResults =
    sampleResultCPList
        .stream()
        .map(CompletableFuture::join)
        .collect(Collectors.toList()))
\end{lstlisting}

\begin{lstlisting}[caption={"AbstractCollector" Create CollectorResult}, captionpos=b, label={lst:abstract-collector-step-three}]
final Map<String, Object> dataMap = Maps.newLinkedHashMap();
sampleResults.forEach(dataMap::putAll);
final CollectorResult collectorResult = new CollectorResult(getCollectorType().name().toLowerCase(),dataMap);
return collectorResult;
\end{lstlisting}

At the end, the CollectorResult containing the type and the data will be generated.

The implementations discussed in coming sections are all based on the concept, to divide the collection into separate units and
to create an overall result by aggregating individual sample data. As a result of this approach, the implementations keeps flexible
what makes it easy to add or remove sample data just by providing implementations of the SampleCollector interface or removing
an item from the internal registry. In addition, as shown above in Code snippet 5.3, the collecting of sample data and the
preparation of the CollectorResult can be realized with just a few lines of code,  which makes the integration of
further data sources quite easy. This can be demonstrated in the comming section.

\subsection{FlinkRestCollector}

This collector implementation fetches data from the HTTP monitoring API that comes with Apache Flink and uses the endpoints
introduced in Table 3.3 in Section 3.1.2 "Application data".

Following the sample approach described above, this collector provides its internal sample registry, containing the SampleCollector
implementations based on the data that is desired and shall be collected. Because the monitoring API provides data concerning
general server and cluster information as well as detailed information about jobs and their state, the registry contains
two appropriate SampleCollectors:

\begin{lstlisting}[caption={"FlinkRestCollector" Sample registry}, captionpos=b, label={lst:flink-rest-collector-sample-registry}]
private static Map<String, SampleCollector> flinkRestSampleRegistry(final FlinkRestClient flinkRestClient) {
    final Map<String, SampleCollector> registry = Maps.newHashMap();
    registry.put(ClusterInfoCollector.SAMPLE_KEY, new ClusterInfoCollector(flinkRestClient));
    registry.put(JobInfoCollector.SAMPLE_KEY, new JobInfoCollector(flinkRestClient));
    return registry;
}
\end{lstlisting}

The implementation of SampleCollectors just differs in the way of data access. While the JvmCollector uses an MBeanServerConnection
to query data using the JMX management interface, FlinkRestCollector is based on a REST client internally to request data, which builds
on the the class RestTemplate, provided by the Spring Framework:

\begin{lstlisting}[caption={"FlinkRestCollector" Rest client}, captionpos=b, label={lst:flink-rest-collector-client}]
final CompletableFuture<OverviewResult> flinkOverviewFuture = restClient().getFlinkOverview();
...
final Map<String, Object> dataMap = Maps.newLinkedHashMap();
dataMap.put(FLINK_JOBS_RUNNING_KEY, flinkOverview.getJobsRunning());
dataMap.put(FLINK_FINISHED_KEY, flinkOverview.getJobsFinished());
dataMap.put(FLINK_CANCELLED_KEY, flinkOverview.getJobsCancelled());
dataMap.put(FLINK_FAILED_KEY, flinkOverview.getJobsFailed());
final Map<String, Object> resultMap = Maps.newLinkedHashMap();
resultMap.put(SAMPLE_KEY, dataMap);
return resultMap;
\end{lstlisting}

By providing this sample data, this collector creates a CollectorResult conaining the data fetched from Apache Flinks monitoring
API without the need to change the main collector process.

\subsection{DStatCollector}

To collect system related data defined in Table 3.1 in Section 3.1.1 "System Data", this implementation uses the Dstat system tool,
that provides multiple parameters to specify the data to be displayed. According to the data analysis, following parameters will be used:

\begin{lstlisting}[caption={"DstatCollector" program parameters in }, captionpos=b, label={lst:dstat-parameters}]
private static final String[] DSTAT_COMMAND = {"dstat", "-t",
    "--cpu", "--top-cpu-adv", "--top-cputime", "--top-cputime-avg",
    "--disk", "--disk-tps", "--disk-util",
    "--net", "--socket", "--tcp", "--udp",
    "--io", "--top-io-adv", "--lock", "--fs",
    "--mem", "--top-mem", "--page", "--swap", "--vm",
    "--sys", "--load", "--ipc", "--unix",
    "--proc", "--proc-count", "--top-latency", "--top-latency-avg",
    "--full",
    "--float", "1", "0"};
\end{lstlisting}

According to the given parameters, DstatCollector is based on the implementations for CpuSampleCollector, DiskSampleColletor, etc.,
which will have to be registrered using a sample registry, as discussed in the sections above.

Starting the Dstat process with the given parameters results in string containing three lines, where only the third line ist required to
to gather data of. To start the external process and to retrieve the resulting output, a ProcessBuilder is used, which is part of
the java.lang package.
\begin{lstlisting}[caption={ProcessBuilder in "DstatCollector"}, captionpos=b, label={lst:dstatprocessbuilder}]
final ProcessBuilder processBuilder = new ProcessBuilder(DSTAT_COMMAND);
processBuilder.redirectErrorStream(true);
final Process process = processBuilder.start();
try (BufferedReader processOutputReader =
    new BufferedReader(new InputStreamReader(process.getInputStream()))) {
        final String dstatResult = processOutputReader.lines()
            .map(String::toString)
            .collect(Collectors.joining(System.lineSeparator()));
        final int exitCode = process.waitFor();
}
\end{lstlisting}

All Dstat sample collectors are based on regular expressions, the third line of the result is splitted, and the data of interest
extracted:
\begin{lstlisting}[caption={"CpuSampleCollector", Extract sample data}, captionpos=b, label={lst:cpusamplecollector}]
final Pattern CPU_USAGE_PATTERN = Pattern.compile("" +
    "(\\d+(\\.\\d+)?)(\\s*)" +
    "(\\d+(\\.\\d+)?)(\\s*)" +
    "(\\d+(\\.\\d+)?)(\\s*)" +
    "(\\d+(\\.\\d+)?)(\\s*)" +
    "(\\d+(\\.\\d+)?)(\\s*)" +
    "(\\d+(\\.\\d+)?)")
...
final Matcher matcher = CPU_USAGE_PATTERN.matcher(raw.trim());
final Map<String, Object> cpuUsageMap = Maps.newLinkedHashMap();
if (!matcher.matches()) {
    LOG.warn("Unable to parse 'CpuUsage'");
} else {
    try {
        cpuUsageMap.put(CPU_NAME_KEY, cpuName);
        cpuUsageMap.put(CPU_USAGE_USER_KEY, Float.valueOf(matcher.group(1)));
        cpuUsageMap.put(CPU_USAGE_SYSTEM_KEY, Float.valueOf(matcher.group(4)));
        ...
        } catch (NumberFormatException ex) {
            LOG.warn("Unable to parse 'CpuUsage'");
        }
}
\end{lstlisting}

\subsection{FlinkJmxController and KafkaBrokerJmxCollector}

The collection of application data using JMX for Apache Flink and Apache Kafka is working in the
same way as discussed in the previous sections. The implementations both provide SampleCollectors for collecting data
from the managed resources which are listed for Apache Kafka in Appendix A by using a MBeanServerConnection and querying the resources by their JMX ObjectName,
see Chapter 2 "Basic Concepts". Since Apache Flink is just providing a very basic set
of application data in its current version, containing rudimentary JVM data like cpu load and basic information about current
jobs and their state, this data will be collected nevertheless. It will be assumed to the metric system will be improved in upcoming versions
of Apache Flink. Furthermore the data is redundant, because the implementation of the JvmCollector and FlinkRestCollector provides a
more detailed set of JVM and job data.

\section{CollectorClient}

\subsection{CollectorWorker}

\subsection{KafkaOutboundWriter}

\subsection{ScheduleController}

\subsection{ClientMetadataController}

\section{CollectorManager}

\subsection{CollectorClientInstanceService}

\subsection{MetadataRestClient}

\subsection{IndexController}

\section{Summary}

%According to the functional requirements
%discussed in Chapter 3 "Requirements and Specification", any other programming language could had been used, as long
%it is possible to provide and consume a RESTfull API and integration for the Producer and Consumer API of Apache
%Kafka is available.

Maybe Spring alternatives, Lagom, VertX, Play?

Maybe collector as agent, Instrumentation instead of separate service

Alternatives REST, maybe (Web-)Sockets

Possible secururity risk because remote JMX, firewalls and dstat process