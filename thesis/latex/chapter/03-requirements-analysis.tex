\chapter{Requirements Analysis and Specification}

After a short introduction to the basic terms and Apache Flink and Apache Kafka in context of stream processing, this chapter
examines what different kind of data are available for both of the systems. According to the results of the data analysis, the
functional and non-functional requirements of the software system which is forming the core of the present thesis will be defined.

\section{Data Analysis}

In preparation of this thesis my supervisor Prof. Dr. Stefan Edlich once said \textit{"Sie sammeln
alles, was nicht bei drei auf dem Baum ist"}. According to this statement, the main focus of the coming section
is the analysis of available data that will be collected by the software solution and not a deeper exploration according
to the relevance and quality of data that is availbale for Apache Flink and Apache Kafka.

\textit{"You can only control what you observe and measure."}\cite{Ebert07}. Even though logfiles, both provided by
Apache Flink and Apache Kafka, are usefull for tracing problems in software systems, problems can be tracked and potential
sources of error can be identified much earlier by collecting and storing system and application data at runtime to
describe the state of the entire system at a given point in time.

Due to the distributed character of Apache Flink and Apache Kafka, where a system is composed
of several interacting components, the examination of log data is not an adequate choice
to gain insight into an entire system \cite{VanL14}.


%The following section describes the sources for system and application data available:

\subsection{System data}

Observation of cpu-, disk- and memory-utilization, why \cite{Hoeb12}.
Dstat system util introduction

To monitor the performance, such as processor, memory, network, and
system utilization. The servers should always in its best performance and should be available when needed. To monitor
all of those components in Linux system, we already know iostat (monitor system input output),
vmstat (monitor memory utilization) and ifstat (monitor network usage). What if you can have 1 tool which has
those 3 functions above and even more? We can do it using dstat tool.





%What to Transport? Logs vs. Metrics see http://blog.mmlac.com/log-transport-with-apache-kafka/
%The first consideration should be if it is possible and/or necessary to transport all logs to a central
%location. If there are many servers or a lot of log data, this might be very resource intensive and aggregating
%or filtering the data might be necessary. The extremes of this are either transporting every single log vs. only
%transporting aggregated metrics. The following paragraphs try to help you decide on the right balance for your use case.
%
%Advantages of transporting all logs:
%
%Metrics can be added, modified and deleted in one central location
%Historical data on new metrics can be computed from the stored logs
%Possibility to peek into live data-stream
%Allows building complex debugging and monitoring tools
%Central location for all logs. Invaluable for debugging, root-cause analysis and correlation of incidents
%Advantages of transporting only metrics:
%
%Transporting (aggregated) metrics requires far less bandwidth
%Smaller storage requirements
%Scales far better
%Better than nothing
%Overall transporting all logs has many advantages and should be preferred over aggregated metrics if possible. Especially managing metric definitions in one place and the ability to compute historic data for new metrics is very valuable. Also does transporting all logs allow for thorough (computationally expensive) data analysis on historic data to i.e. train machine learning models, predict behavior or give enhanced insights into who your users are and what they do.
%
%There is no strict rule to follow and it is perfectly ok to mix and match. An example would be to just transport logs that contain examinable data and aggregate performance metrics, like average response time or jobs processed per minute, on the server.
%
%This post will focus on the transport of raw log data. The posts “Server Monitoring with Sensu” and “Metrics with Graphite” will introduce better suited technologies to work with pure metrics.
\subsection{Application data}

Apache Flink provides application data via Monitoring REST API, describe REST
Since version 1.1.0 new Metrics data via JMX
Analyze Flinks REST data



KORRELATION sytem and application data

\section{Data Quality}

Define DQ, evaluate quality for data above

\section{Functional Requirements}

The main objective of this bachelor thesis is to implement a software solution to collect ....
"COLLECT EVERYTHING!
live and historical sources

Describe "big picture" functionality see \cite{VanL14}, follows distributed character of Big Data Analytics
Applications, provide "on demand" data collection, as much data as possible, realtime?, three main components,
break down for:

\subsection{Collection}

collect data in clustered environments

\subsection{Transport}

Scalability with message broker

\subsection{Persistence}

Accessibility for AI, UI applications

\section{Non-Functional Requirements}

Performance, scalability,
%simplicity, modifiability, visibility, portability, and reliability

\section{Summary}