\chapter{Basic Concepts}
After a short introduction to the terminology of Big Data and Big Data Analytics Applications
this chapter introduces the concept of stream processing, which is one of the main characteristics
of the streaming frameworks Apache Flink and Apache Kafka. The underlying concepts both of these systems
and their usage in context of Big Data Analytics will be explained at the end of this chapter.
TODO REST JMX

\section{Big Data}
In the past decade the amount of data being created is a subject of immense growth.
More than 30,000 gigabytes of data are generated every second, and the rate of data
creation is accelerating\cite{Marz15}. People create content like blog posts, tweets, social
network interactions, photos, servers continuously log messages, scientists create detailed
measurements, permanently.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.55\textwidth]{../images/04-sources-of-bigdata.png}
	\caption{Sources of Big Data{\cite{Bitk12}}}
	\label{sources-of-bigdata}
\end{figure}

Through advances in communications technology, people and things are becoming in-
creasingly interconnected. Generally titled as machine-to-machine (M2M), inter-
connectivity is responsible for double-digit year over year data growth rates. Finally,
because small integrated components are now affordable, it becomes possible to add
intelligence to almost everything. As an example, a simple railway car has hundreds
of sensors for tracking the state of individual parts and GPS-based data for shipment
tracking and logistics\cite{Ziko12}.

Besides the extremely growing amount of data, the data becomes more and more diverse.
It exists in its raw and unstructured, semistructured or in rare cases in a structured form.
\cite{Bitk12} describes, that around 85 percent of the data comes in an unstructured
form, but containing valuable information what makes processing it in a traditional
relational system impractical or impossible.

According to \cite{Marz15} \cite{Ziko12}, Big Data is defined by three characteristics:
\begin{description}
    \item [Volume] The amount of present data because of growing amount of producers,
    e.g. environmental data, financial data, medical data, log data, sensor data.
    \item [Variety] Data varies in its form, it comes in different formats from different sources.
    \item [Velocity] Data needs to be evaluated and analyzed quickly, which leads to new challenges
    of analyzing large data sets in seconds range or processing of data in realtime
\end{description}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{../images/03-three-vs-of-bigdata.png}
	\caption{The three 'V's of Big Data{\cite{Ziko12}}}
	\label{three-vs-of-bigdata}
\end{figure}

A possible definition for Big Data could be derived as follows: \textit{Big Data refers to the use
of large amounts of data from multiple sources with a high processing speed for generating
valuable information based on the underlying data.}

Another definition comes from the the science historian George Dyson, who was cited by
Tim O'Reilly in \cite{Dys13}:
\textit{Big data is what happened when the cost of storing information became less than the
cost of making the decision to throw it away.}

According to \cite{Marz15} the term “Big Data” is a misleading name since it implies that
pre-existing data is somehow small, which is not true, or that the only challenge is the
sheer size of data, which is just one one them among others. In reality, the term Big Data
applies to information that can’t be processed or analyzed using traditional processes or
tools.

\section{Big Data Analytics Applications}
Big Data Analytics describes the process of collecting, organizing and analyzing large
volumes of data with the aim to discover patterns, relationships and other useful informa-
tion extracted from incoming data streams \cite{Marz15}. The process of analytics is typically
performed using specialized software tools and applications for predictive analytics, data
mining, text mining, forecasting and data optimization.

The analytical methods raise data quality for unstructured data on a level that allows
more quantitative and qualitative analysis. With this structure it becomes possible
to extract the data that is relevant for more detailed queries to extract the desired information.

The areas of applications may be extremely diverse and ranges from analysis of financial
flows or traffic data, processing sensor data or environmental monitoring as explained in
the previous chapter.

The illustration below summarises the six-dimensional taxonomy \cite{Bitk14, Csa14} of Big
Data Analytics Applications.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{../images/05-big-data-taxonomy.jpg}
	\caption{Taxonomy of Big Data Analytics Applications \cite{Bitk14, Csa14}}
	\label{taxonomy-bigdata-applications}
\end{figure}

The following section will discus the topic stream processing, which is part of the
"Compute infrastructure" layer shown in the figure above.

\section{Stream Processing}

Computing infrastructures on big data currently differ on whether the processing on streaming data
will be computed in batch mode, or in real-time/near real-time . This section is focussed on processing
continous data streams in real-time/near real-time and introduces Apache Flink and Apache Kafka as
representants of streaming frameworks.

According to \cite{Klepp16}, stream processing is the computing of data continuously,
concurrently, in real time and in a record-by-record fashion. In a stream, data isn't as treated static tables
or files, but as a continuous infinite stream of data extracted from both live and historical
sources. Various data streams could have own features, for example, a stream
from the financial market describes the whole data. In the same time, a stream for sensors
depends on sampling (e.g. get new data every 5 minutes).

The general approach is to have a small component that processes each of the events
separately. In order to speed up the processing, the stream may be subdivided, and the
computation distributed across clusters. Stream processing frameworks like Apache Flink and
Apache Kafka primarily addresses parallelization of the computational load, an additional storage
layer is needed to store the results in order to be able to query them.

This continous processing of data streams leads to the benefits of stream processing frameworks:

\begin{itemize}
	\item Accessibility: live data can be used while the data flow is still in motion and before the data being stored.
	\item Completeness: historical data can be streamed and integrated with live data for more context.
	\item High throughput: large volumes of data can be processed in high-velocity with minimal latency.
\end{itemize}

To introduce a more formal expression, a data stream is described as an ordered pair (S, T) where:
\begin{itemize}
	\item S is a sequence of tuples.
	\item T is a sequence of positive real time intervals.
\end{itemize}

It defines a data stream as a sequence of data objects, where the sequence in a data stream
is potentially unbounded, which means that data streams may be continuously generated
at any rate \cite{Nam15} and leads to the following characteristics:
\begin{itemize}
	\item the data arives continous
	\item the arrival of data is disordered
	\item the size of the stream is potentially unbounded
\end{itemize}

After this short introduction to the basics of stream processing, the following sections
covers a short introduction of the streaming frameworks Apache Flink and Apache Kafka.
\subsection{Apache Flink}

As described in the documentation \cite{Flink16}, \textit{"Apache Flink is an open source platform for
distributed stream and batch data processing. Flink’s core is a streaming dataflow engine that
provides data distribution, communication, and fault tolerance for distributed computations over
data streams. Flink also builds batch processing on top of the streaming engine, overlaying native
iteration support, managed memory, and program optimization."}

The main components of Flink applications are formed by streams and transformations, in which
streams define intermediate results whereas transformations represent operations computed on one
or more input streams with one or more resulting streams.

To illustrate the main components of a Flink application, the following code from \cite{Flink16} shows
a working example of a streaming application, that counts the words coming from a web socket in 5
second windows:

\begin{lstlisting}[caption={Basic Apache Flink streaming application}, captionpos=b, label={lst:basicflink}]
public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        DataStream<Tuple2<String, Integer>> dataStream = env
                .socketTextStream("localhost", 9999)        *(1)
                .flatMap(new Splitter())                    *(2)
                .keyBy(0)                                   *(2)
                .timeWindow(Time.seconds(5))                *(2)
                .sum(1);                                    *(2)

        dataStream.print();                                 *(3)
        env.execute("Window WordCount");
    }

    public static class Splitter implements FlatMapFunction<String, Tuple2<String, Integer>> {
        @Override
        public void flatMap(String sentence, Collector<Tuple2<String, Integer>> out) throws Exception {
            for (String word: sentence.split(" ")) {
                out.collect(new Tuple2<String, Integer>(word, 1));
            }
        }
    }
\end{lstlisting}

On execution, Flink applications are mapped to streaming dataflows, consisting of streams
and transformation operators (3) where each dataflow starts with one or more sources (1)
the data is received from and the resulting stream will be written in one or more sinks (3).
to.

The dataflows of Apache Flink are in inherently parallel and distributed, by splitting streams into
stream partitions and operators into operator subtasks, which are execute independently from each
other, in different threads and on different machines or containers.

For the distributed processing of dataflows, Flink defines two type of processes:

\begin{enumerate}
    \item JobManagers (master process). At least one is required, it coordinates the
    distributed execution and is responsible for scheduling tasks, coordinate recovery
    on failures, etc.
    \item TaskManagers (worker processes) At least one is required, it executes the tasks, more
    specifically, the subtasks of a dataflow, and buffer and exchange the data streams.
\end{enumerate}

A basic Flink cluster set up with a single JobManager and TaskManager on Docker will be introduced in
Chapter 5 Evaluation and serves as source to collect data from, as well as a streaming component for
processing collected data.

In addition, Apache Flink provides a client, which is not part of the runtime. It is used as a part
of Java/Scala applications to create and send dataflows to the JobManager. The client will
be used in the software component "CollectorDataProcessor" and introduced in Chapter 4 Architecture
and Implementation.

\subsection{Apache Kafka}

Apache Kafka is publish-subscribe queuing service rethought as a distributed commit log \cite{Kafka16},
supporting stream processing with millions of messages per second, durability of messages through disk
storage and replication accross multiple machines in clustered environments. It is written in Scala, was
initially developed at LinkedIn and follows the distributed character of Big Data Analytics Applications
by it's inherent design.

This excerpt from the paper \cite{Neha11} the team at LinkedIn published about Kafka describes the
basic principles:

\textit{A stream of messages of a particular type is defined by a topic. A producer can publish
messages to a topic. The published messages are then stored at a set of servers called brokers.
A consumer can subscribe to one or more topics from the brokers, and consume the subscribed
messages by pulling data from the brokers. (…) To subscribe to a topic, a consumer first creates
one or more message streams for the topic. The messages published to that topic will be evenly
distributed into these sub-streams. (…)  Unlike traditional iterators, the message stream iterator
never terminates. If there are currently no more messages to consume, the iterator blocks until
new messages are published to the topic.}

A common use case for Apache Kafka in the context of stream processing is the buffering of messages
between stream producing systems by providing a queue for incoming and outgoing data. According
to the explanation of the concept of data sources and sinks in the Apache Flink section above, Apache Kafka
is heavily used as an input source, as well as output sink for the processing dataflow in Apache Flink
applications.

The following figure shows a typical use case for a data pipeline that typically start by pushing data
streams into Kafka, consumed by Flink applications, which range from simple data transformations
to complex data aggregations in a given time window. The resulting streams are written back to Kafka
for the consumption by other services or the storage in a persitent medium.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{../images/05-kafka-flink-pipeline.png}
	\caption{A typical Kafka-Flink pipeline{\cite{Dartisans15}}}
	\label{kafka-flink-pipeline}
\end{figure}

Chapter 5 Evaluation describes the Docker setup for a single Kafka node that is part of the
software solution in addition of provisioning data for collection.

\section{Representational State Transfer (REST)}
In his doctoral dissertation from 2000 titled "Architectural Styles and the Design of Network-based
Software Architectures", Roy Thomas Fiedling introduced the term Representational
State Transfer (REST) as core set of principles, properties, and constraints defining an "architectural
style for distributed hypermedia systems"\cite{Field00}.

Focussed on component roles and interactions between data elements rather than implementation details,
Fielding defines a set of core principles of REST architectures. The following section discusses the relavant
parts according to the thesis:
\begin{enumerate}
    \item \textbf{Client-Server}
    Clients and servers are separated by a uniform interface to facilitate portability. For example, a user interface
    is not concerned with data storage because is internal to the server. On the other hand, the server is not concerned
    with the user interface or state. As long as the interface is not altered, the separation of concerns enables the
    components to evolve independently and thereby the improves scalability of the entire system.
    \item \textbf{Stateless}
    The communication between clients and server must be stateless. Each request from any client contains all the
    information necessary to service the request, and session state is held in the client.
    \item \textbf{Uniform Interface}
    The uniform interface between the interacting components is a fundamental characteristic of REST architectures and subjects
    to the following constraints:
    \begin{enumerate}
        \item identification of resources
        \item manipulation of resources through representations
        \item self-descriptive messages
        \item hypermedia as the engine of application state
    \end{enumerate}
%    \item \textbf{Caching}
%    A response to a request be implicitly or explicitly defined as cacheable or non-cacheable.
%    If a response is cacheable, a client cache can reuse that response data for later, equivalent requests,
%    and therefore eliminates some client-server interactions.
%    , improving efficiency, scalability, performance by
%    reducing the average latency of a series of interactions.
\end{enumerate}



%Bei einer RESTful API handelt es sich um eine Programmierschnittstelle (Application Programming Interface, API),
%die HTTP-Anfragen verwendet, um per GET, PUT, POST und DELETE auf Daten zuzugreifen.

%RESTful APIs machen sich explizit die in RFC 2616 definierten HTTP-Anfragemethoden zunutze. Sie verwenden PUT, um den
%Zustand einer Ressource (das kann ein Objekt, eine Datei oder ein Block sein) zu ändern oder sie zu aktualisieren.
%Mit GET wird eine Ressource abgerufen, mit POST wird sie erstellt und mit DELETE gelöscht.



%Als Internetprotokoll sollte HTTP/1.1
%dienen. Über URIs adressierbare Informationsbündel bezeichnet man Ressourcen, welche über
%Repräsentations- beziehungsweise. Darstellungsmöglichkeiten verfügen und Meta Daten enthalten. 71 Man
%denkt in Ressourcen. Aber was ist eine Ressource? Jede Ressource ist über eine eindeutige URI
%identifizierbar und gilt als Inhalt, welcher bei der Web Applikation liegt. Ressourcen können Sub Ressourcen
%haben, worauf keine oder mehrere Methoden anwendbar sind. Neben der freien Dokumentation des Web
%Services, wird oft die in XML verfügbare Web Application Description Language (WADL) des RESTful
%Services veröffentlicht. 72 Da REST auf HTTP basiert, werden auch die dort bekannte    n idempotenten
%Methoden wie GET, PUT, POST, DELETE verwendet. Idempotenz bedeutet in diesem Zusammenhang, dass
%keine Nebeneffekte auftreten, wenn eine HTTP Anfrage öfter als einmal ausgeführt wird. Weiterhin werden die
%durch das offene Internetprotokoll bekannten HTTP Status Codes für Antworten verwendet. 73 Neben der von
%Fielding erwähnten Repräsentationen als HTML Dokument und Image 74 existieren heute eher XML und JSON,
%wobei binäre Daten nicht zu vergessen sind.

\section{{Java Management Extensions (JMX)}}

General JMX explanation, data access MBeansServerConnection

\section{Summary}

TODO