\chapter{Theoretical Foundations}

This chapter will discuss the main characteristics of Big Data Analytics Applications and
introduces the concept of stream processing, which is one of the main characteristics of the
popular streaming frameworks Apache Flink and Apache Kafka. The underlying concepts both of
these systems and how they're used in context of Big Data Analytics will be explained at the
end of this chapter.

\section{Big Data Analytics Applications}

Big Data Analytics describes the process of collecting, organizing and analyzing large volumes
of data with the aim to discover patterns and other useful information extracted from a incoming
data streams \cite{Marz15}. The process of analytics is typically performed using specialized software tools and
applications for predictive analytics, data mining, text mining, forecasting and data optimization.

The areas of applications may be extremely diverse and ranges from analysis of financial flows or
traffic data, processing sensor data or environmental monitoring.

Characteristics:
\begin{description}
    \item [Robustness and fault tolerance] TODO
    \item [Low latency reads and updates] TODO
    \item [Generalization] TODO
    \item [Ad hoc queries] TODO
\end{description}

\section{Stream-Processing}
According to \cite{Klepp16}, stream processing is the real-time processing of data continuously,
concurrently, and in a record-by-record fashion in which data is treated not as static tables
or files, but as a continuous infinite stream of data integrated from both live and historical
sources.

Benefits:
\begin{itemize}
	\item Accessibility: live data can be used while still in motion, before being stored.
	\item Completeness: historical data can be streamed and integrated with live data for more context.
	\item High throughput: high-velocity, high-volume data can be processed with minimal latency.
\end{itemize}

\subsection{Apache Flink}

\subsection{Apache Kafka}

\subsection{Related work}

\subsubsection{Prometheus}

\subsubsection{Datadog}

\subsubsection{New Relic}

\subsubsection{collectd}

collectd is a daemon which collects system performance statistics periodically and
provides mechanisms to store the values in a variety of ways, for example in RRD files.

\subsubsection{collectd}

StatsD is originally a simple daemon developed and  released by Etsy  to aggregate and summarize application metrics.
With StatsD, applications are to be instrumented by developers using language-specific client libraries. These libraries
will then communicate with the StatsD daemon using its dead-simple protocol, and the daemon will then generate aggregate
metrics and relay them to virtually any graphing or monitoring backend.

\section{Summary}